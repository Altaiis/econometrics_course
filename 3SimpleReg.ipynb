{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1> Introductory Econometrics in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Table of Content ##\n",
    "\n",
    "1. Welcome and Introduction\n",
    "2. The Nature of Ecomometrics and Economic Data\n",
    "3. ***Simple Regression with Cross-sectional Data***\n",
    "4. Multiple Regression with cross-sectional Data, including Inference and Hypothesis testing\n",
    "5. Binary Dependent Variables\n",
    "6. Regression Analysis with Panel Data\n",
    "7. Estimation of Treatment Effects: Difference-in-difference Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple Regression with Cross-sectional Data #\n",
    "1. Concept\n",
    "2. The Ordinary Least Squares Estimator (OLS)\n",
    "3. Goodness of Fit\n",
    "4. Unit of Measurement and Functional Forms\n",
    "5. Expected Values and Variances of OLS\n",
    "6. Binary Variables\n",
    "7. Counterfactual Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Concept of the Simple Regression Model\n",
    "\n",
    "Much of applied econometric analysis begins with the following premise: \n",
    "\n",
    "<blockquote> \n",
    "\n",
    "$y$ and $x$ are two variables, representing some population (e.g., individuals, firms, regions), and we are interested in “explaining $y$ in terms of $x$,” or in “studying how $y$ varies with changes in $x$.”\n",
    "    \n",
    "\n",
    "</blockquote>\n",
    "\n",
    "Some examples include:\n",
    "- $y$ is  *soybean crop yield* and $x$ is amount of fertilizer;\n",
    "- $y$ is *hourly wage* and $x$ is *years of education*;\n",
    "- $y$ is a community *crime rate* and $x$ is number of police officers.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Simple Regression Model\n",
    "\n",
    "In writing down a model that will “explain $y$ in terms of $x$,” we confront three issues. \n",
    "\n",
    "1. Because there is never an exact relationship between two variables, how do we allow for other factors to affect $y$?\n",
    "2. What is the functional relationship between $y$ and $x$? \n",
    "3. How can we ensure to capture a ceteris paribus relationship between $y$ and $x$ (if that is a desired goal)?  \n",
    "      \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Simple Regression Model**\n",
    "      \n",
    "- We can resolve these ambiguities by writing down an equation relating $y$ to $x$.\n",
    "- A simple equation is  \n",
    "  \n",
    "  \n",
    "\n",
    "\\begin{equation*}\n",
    "y=\\beta_0+\\beta_1x+u\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "- The above equation, which is assumed to hold in the population of interest, defines the **simple linear regression model**.\n",
    "- It is also called the two-variable linear regression model or bivariate linear regression model because it relates the two variables x and y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Simple Regression Model**\n",
    "\n",
    "\n",
    "| Y | X |\n",
    "| :--- | :--- |\n",
    "| Dependent variable | Independent variable |\n",
    "| Explained variable | Explanatory variable |\n",
    "| Response variable | Control variable |\n",
    "| Predicted variable | Predictor variable |\n",
    "|  Regressand | Regressor |\n",
    "| Outcome variable | Variable of interest |\n",
    "    \n",
    "- The variable $u$, called the **error term** or **disturbance** in the relationship, represents factors other than $x$ that affect $y$.\n",
    "- A simple regression analysis effectively treats all factors affecting $y$ other than $x$ as being unobserved.\n",
    "- You can usefully think of $u$ as standing for “unobserved.” (sometimes you also see $e$, which you can think of as \"error\".)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Simple Regression Model**\n",
    "\n",
    "- The equation $y=\\beta_0+\\beta_1x+u$ also addresses the issue of the functional relationship between $y$ and $x$.\n",
    "- If the other factors in $u$ are held fixed, so that the change in $u$ is zero, $\\Delta{u}$= 0, then $x$ has a linear effect on $y$:  \n",
    "  \n",
    "  \n",
    "<center> $\\Delta{y}=\\beta_1\\Delta{x}$ if $\\Delta{u}$=0. </center>\n",
    "\n",
    "- Thus, the change in $y$ is simply $\\beta_1$ multiplied by the change in $x$. This means that $\\beta_1$ is the **slope parameter** in the relationship between $y$ and $x$, holding the other factors in $u$ fixed; it is of primary interest in applied economics. \n",
    "- The **intercept parameter** $\\beta_0$, sometimes called the **constant term**, also has its uses, although it is rarely central to an analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example: Soybean Yield and Fertilizer**  \n",
    "  \n",
    "<blockquote> Suppose that soybean yield is determined by the model:  \n",
    "    \n",
    "<br>\n",
    "      \n",
    "\\begin{equation}\n",
    "yield=\\beta_0+\\beta_1 fertilizer + u\n",
    "\\end{equation}\n",
    "\n",
    "  \n",
    "where $y$= _yield_ and $x$=_fertilizer_.\n",
    "\n",
    "- The agricultural researcher is interested in the effect of fertilizer on yield, holding other factors fixed. This effect is given by $\\beta_1$.\n",
    "- The error term u contains factors such as land quality, rainfall, and so on.\n",
    "- The coefficient $\\beta_1$ measures the effect of fertilizer on yield, holding\n",
    "other factors fixed:  \n",
    "\n",
    "<br>\n",
    "\n",
    "<center> $\\Delta{yield}=\\beta_1\\Delta fertilizer$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example: A Simple Wage Equation**  \n",
    "  \n",
    "<blockquote> A model relating a person's wage to observed education and other unobserved factors is  \n",
    "    \n",
    "<br>\n",
    "      \n",
    "\\begin{equation}\n",
    "wage=\\beta_0+\\beta_1 educ + u\n",
    "\\end{equation}\n",
    "\n",
    "- _wage_ is measured in dollars per hour and $educ$ is years of education.\n",
    "- $\\beta_1$ measures the change in hourly wage given another year of education, holding all other factors fixed.\n",
    "- Some of those factors include labor force experience, innate ability, tenure with current employer, work ethic, and numerous other things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Simple Regression Model**\n",
    "\n",
    "- The linearity of equation $y=\\beta_0+\\beta_1x+u$ implies that a one-unit change in $x$ has the same effect on $y$, regardless of the initial value of $x$.\n",
    "    - Unrealistic for many economic applications.\n",
    "    - For example, in the wage-education example, we might want to allow for increasing returns: the next year of education has a larger effect on wages than did the previous year.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Simple Regression Model**\n",
    "\n",
    "\n",
    "- We will see how to allow for such possibilities later.\n",
    "-  The most difficult issue to address is whether the above model really allows us to draw ceteris paribus conclusions about how $x$ affects $y$.\n",
    "- We just saw in equation $\\Delta{y}=\\beta_1\\Delta{x}$ that $\\beta_1$ does measure the effect of $x$ on $y$, holding all other factors (in $u$) fixed.\n",
    "- Is this the end of the causality issue? Unfortunately, **NO**!\n",
    "- We will learn later why!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Simple Regression Model**\n",
    "\n",
    "- How can we learn about the ceteris paribus effect of $x$ on $y$, holding other factors fixed, when we are ignoring all those other factors?\n",
    "- We are only able to get reliable estimators of $\\beta_0$ and $\\beta_1$ from a random sample of data when we make an assumption restricting how the unobservable $u$ is related to the explanatory variable $x$.\n",
    "- Without such a restriction, we will not be able to estimate the ceteris paribus effect, $\\beta_1$.\n",
    "- Before we state the key assumption about how $x$ and $u$ are related, we can always make one assumption about $u$:\n",
    "\n",
    "    - As long as the intercept $\\beta_0$ is included in the equation, nothing is lost by assuming that the average value of $u$ in the population is zero. \n",
    "    - Mathematically,\n",
    "\n",
    "\\begin{equation*}\n",
    "E(u)=0\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Simple Regression Model**\n",
    "\n",
    "- Assumption $E(u)=0$ says nothing about the relationship between $u$ and $x$, but simply makes a statement about the distribution of the unobserved factors in the population.\n",
    "- Using the previous examples for illustration, we can see that assumption $E(u)=0$ is not very restrictive.\n",
    "    - In the soybean example, we lose nothing by normalizing the unobserved factors affecting soybean yield, such as land quality, to have an average of zero in the population of all cultivated plots.\n",
    "    - The same is true of the unobserved factors in the wage example.\n",
    "    - Without loss of generality, we can assume that things such as average ability are zero in the population of all working people.\n",
    "- We can always redefine the intercept in $y=\\beta_0+\\beta_1x+u$ to make $E(u)=0$ true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Simple Regression Model**\n",
    "\n",
    "\n",
    "We now turn to the crucial assumption regarding how $u$ and $x$ are related:\n",
    "\n",
    "- A natural measure of the association between two random variables is the _correlation coefficient_.\n",
    "- If $u$ and $x$ are uncorrelated, then, as random variables, they are not _linearly_ related.\n",
    "- Assuming that $u$ and $x$ are _uncorrelated_ goes a long way toward defining the sense in which $u$ and $x$ should be unrelated in equation $y=\\beta_0+\\beta_1x+u$.\n",
    "- But it does not go far enough, because correlation measures only linear dependence between $u$ and $x$.\n",
    "- Correlation has a somewhat counterintuitive feature: it is possible for $u$ to be uncorrelated with $x$ while being correlated with functions of $x$, such as $x^2$.\n",
    "- This possibility is not acceptable for most regression purposes, as it causes problems for interpreting the model and for deriving statistical properties.\n",
    "    \n",
    "A better assumption involves the _expected value of $u$ given $x$_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Simple Regression Model**\n",
    "\n",
    "- Because $u$ and $x$ are random variables, we can define the conditional distribution of $u$ given any value of $x$.\n",
    "- In particular, for any $x$, we can obtain the expected (or average) value of $u$ for that slice of the population described by the value of $x$.\n",
    "- The crucial assumption is that the average value of $u$ does not depend on the value of $x$.\n",
    "- We can write this assumption as\n",
    "\n",
    "\\begin{equation*}\n",
    "E(u|x)=E(u)\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Simple Regression Model**\n",
    "\n",
    "$E(u|x)=E(u)$ says that the average value of the unobservables is the same across all slices of the population determined by the value of $x$ and that the common average is necessarily equal to the average of $u$ over the entire population.\n",
    "- When assumption $E(u|x)=E(u)$ holds, we say that $u$ is **mean independent** of $x$. \n",
    "- When we combine mean independence with assumption $E(u)=0$, we obtain the **zero conditional mean assumption**, $E(u|x)=0$.\n",
    "- It is critical to remember that equation $E(u|x)=E(u)$ is the assumption with impact; assumption $E(u)=0$ essentially defines the intercept, $\\beta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Simple Regression Model**\n",
    "\n",
    "Let us see what equation $E(u|x)=E(u)$ entails in the wage example.\n",
    "- To simplify the discussion, assume that $u$ is the same as innate ability (or intelligence).\n",
    "- Then equation $E(u|x)=E(u)$ requires that the average level of ability is the same, regardless of years of education $x$.\n",
    "    - For example, if $E(abil|8)$ denotes the average ability for the group of all people with eight years of education,\n",
    "    - and $E(abil|16)$ denotes the average ability among people in the population with sixteen years of education,\n",
    "    - then equation $E(u|x)=E(u)$ implies that these must be the same.\n",
    "- In fact, the average ability level must be the same for all education levels.\n",
    "\n",
    "Is this a reasonable assumption?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Simple Regression Model**\n",
    "\n",
    "If, for example, we think that people with more ability choose, on average, to become more educated, then equation $E(u|x)=E(u)$ is false.\n",
    "- As we cannot observe innate ability, we have no way of knowing whether or not average ability is the same for all education levels.\n",
    "- But: this is an issue that we must address before relying on simple regression analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Simple Regression Model**\n",
    "\n",
    "- In the fertilizer example, if fertilizer amounts are chosen independently of other features of the plots, then equation $E(u|x)=E(u)$ will hold: the average land quality will not depend on the amount of fertilizer.\n",
    "\n",
    "- Can you think of a situation in which $E(u|x)=E(u)$ would fail in this example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Simple Regression Model**\n",
    "\n",
    "- For instance, if more fertilizer is put on the higher-quality plots of land, then the expected value of $u$ changes with the level of fertilizer, and equation $E(u|x)=E(u)$ fails.\n",
    "- The zero conditional mean assumption gives $\\beta_1$ another interpretation that is often useful.\n",
    "- Taking the expected value of equation $y=\\beta_0+\\beta_1x+u$ conditional on $x$ and using $E(u|x)=0$ gives:  \n",
    "\n",
    "<center>$E(y|x)=\\beta_0+\\beta_1x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Simple Regression Model**\n",
    "\n",
    "- Equation $E(y|x)=\\beta_0+\\beta_1x$ shows that the **population regression function (PRF)**, $E(y|x)$, is a linear function of $x$.\n",
    "- The linearity means that a one-unit increase in $x$ changes the _expected value_ of $y$ by the amount $\\beta_1$.\n",
    "- For any given value of $x$, the distribution of $y$ is centered about $E(y|x)$.\n",
    "- It is important to understand that equation $E(y|x)=\\beta_0+\\beta_1x$ tells us how the _average_ value of $y$ changes with $x$; it does not say that $y$ equals $\\beta_0+\\beta_1x$ for all units in the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$E(y|x)$ as a linear function of $x$**\n",
    "\n",
    "\n",
    "<br>  \n",
    "\n",
    "\n",
    "<center><img src=\"figs/wool_2_1.png\" width=\"500\"/> \n",
    "    \n",
    "\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Simple Regression Model**\n",
    "\n",
    "For example, suppose that $x$ is the high school grade point average and $y$ is the college GPA, and we happen to know that \n",
    "\n",
    "\\begin{equation}\n",
    "E(colGPA|hsGPA)=1.5+0.5 hsGPA\n",
    "\\end{equation}\n",
    "\n",
    "- This GPA equation tells us the average college GPA among all students who have a given high school GPA.\n",
    "\n",
    "So suppose that _hsGPA_=3.6.\n",
    "\n",
    "- Then the average _colGPA_ for all high school graduates who attend college with hsGPA=3.6 is 1.5+0.5(3.6)=3.3.\n",
    "- We are certainly not saying that every student with hsGPA=3.6 will have a 3.3 college GPA; this is clearly false.\n",
    "- The PRF gives us a relationship between the average level of $y$ at different levels of $x$.\n",
    "- Some students with hsGPA=3.6 will have a college GPA higher than 3.3, and some will have a lower college GPA.\n",
    "- Whether the actual _colGPA_ is above or below 3.3 depends on the unobserved factors in $u$, and those differ among students even within the slice of the population with hsGPA=3.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Simple Regression Model**\n",
    "\n",
    "Given the zero conditional mean assumption $E(y|x)=0$, it is useful to view equation $y=\\beta_0+\\beta_1x+u$ as breaking $y$ into two components.\n",
    "- The piece $\\beta_0+\\beta_1s$, which represents $E(y|x)$, is called the _systematic part_ of $y$ - that is, the part of $y$ explained by $x$\n",
    "- and $u$ is called the _unsystematic part_, or the part of $y$ not explained by $x$. \n",
    "\n",
    "Later, when we introduce more than one explanatory variable, we willdiscuss how to determine how large the systematic part is relative to the unsystematic part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Ordinary Least Squares Estimator**\n",
    "\n",
    "Now that we have discussed the basic ingredients of the simple regression model, we will address the important issue of how to estimate the parameters $\\beta_0$ and $\\beta_1$ in equation $y=\\beta_0+\\beta_1x+u$.\n",
    "- To do this, we need a sample from the population.\n",
    "- Let {($x_i,y_i):i=1,...,n$} denote a random sample of size $n$ from the\n",
    "population.\n",
    "- Because these data come from equation $y=\\beta_0+\\beta_1x+u$, we can write $y_i=\\beta_0+\\beta_1x_i+u_i$ for each $i$.\n",
    "- Here, $u_i$ is the error term for observation $i$ because it contains all factors affecting $y_i$ other than $x_i$.\n",
    "- As an example, $x_i$ might be the annual income and $y_i$ the annual savings for family $i$ during a particular year.\n",
    "- If we have collected data on 15 families, then _n=15_.\n",
    "- A scatterplot of such a data set is given in the figure below, along with the (necessarily fictitious) population regression function.\n",
    "- We must decide how to use these data to obtain estimates of the intercept and slope in the population regression of savings on income."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Scatterplot of savings and income for 15 families, and the population regression $E(savings|income)=\\beta_0+\\beta_1 income$**  \n",
    "\n",
    "<br>  \n",
    "\n",
    "\n",
    "<center><img src=\"figs/wool_2_2.png\" width=\"500\"/> \n",
    "    \n",
    "\n",
    "<p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Ordinary Least Squares Estimator**\n",
    "\n",
    "The most basic **estimator** is the **Ordinary Least Square (OLS) estimator**.\n",
    "- The idea is to choose the parameter $\\beta$ that minimizes the squared sum of the error $u_i$ of the model\n",
    "- But we never observe $u_i$ that is unknown to us\n",
    "\n",
    "Instead we have the **sample regression model**:\n",
    "\n",
    "$y_i=\\hat{y}_i + \\hat{u}_i = \\hat{\\beta_0}+ \\hat{\\beta_1}x_{1i}+ \\hat{\\beta_2}x_{2i}+ \\hat{\\beta_3}x_{3i}+ \\hat{u}_i$\n",
    "\n",
    "- $\\hat{y}_i$ are the predicted values, $\\hat{\\beta}$ are estimated coefficients, and $\\hat{u}_i$ are residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Ordinary Least Squares Estimator**\n",
    "\n",
    "- Assume the **population regression model**:  \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "y_i = \\beta_{0} + \\beta_{1}x_{1i} + u_i\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "- The OLS estimator has the following objective function:\n",
    "- $(\\hat{\\beta}_{0}^{OLS}, \\hat{\\beta}_{1}^{OLS})=\\underset{(\\hat{\\beta}_{0}, \\hat{\\beta}_{1})}{argmin[\\sum_i\\hat{u}_i^2]}=\\underset{(\\hat{\\beta}_{0}, \\hat{\\beta}_{1})}{argmin[\\sum_i(\\hat{y}_i-y_i)^2]}$\n",
    "- The algorithm chooses $(\\hat{\\beta}_{0}, \\hat{\\beta}_{1})$ until $\\hat{u}_i^2$ reaches a global minimum\n",
    "- The OLS estimator can also be solved analytically as we will show next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Derivation of the OLS Estimator (Self Study)**\n",
    "\n",
    "- An implication from assumption $E(u|x)=E(u)$ is that in the population $u$ is uncorrelated with $x$.\n",
    "- Combined with $E(u)=0$ we get that the covariance between $x$ and $u$ is zero.\n",
    "\n",
    "\\begin{equation*}\n",
    "Cov(x,u)=E(xu)=0\n",
    "\\end{equation*}\n",
    "\n",
    "- In terms of the observable variables $x$ and $y$ and the unknown parameters $\\beta_0$ and $\\beta_1$, the two equations $E(u)=0$ and $Cov(x,u)=E(xu)=0$ can be written as\n",
    "\n",
    "\\begin{equation*}\n",
    "E(y-\\beta_0-\\beta_1x)=0\n",
    "\\end{equation*}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation*}\n",
    "E[x(y-\\beta_0-\\beta_1x)]=0\n",
    "\\end{equation*}\n",
    "\n",
    "- The two above equations imply two restrictions on the joint probability distribution of $(x,y)$ in the population.\n",
    "- Since there are two unknown parameters to estimate, we might hope that equations the two equations can be used to obtain good estimators of $\\beta_0$ and $\\beta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Derivation of the OLS Estimator (Self Study)**\n",
    "\n",
    "- The respective _sample_ counterparts of the two above equations are\n",
    "\n",
    "\\begin{equation*}\n",
    "n^{-1}\\sum_{i=1}^n(y_i-\\hat\\beta_0-\\hat\\beta_1x_i)=0\n",
    "\\end{equation*}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation*}\n",
    "n^{-1}\\sum_{i=1}^nx_i(y_i-\\hat\\beta_0-\\hat\\beta_1x_i)=0\n",
    "\\end{equation*}\n",
    "\n",
    "- The equation $n^{-1}\\sum_{i=1}^n(y_i-\\hat\\beta_0-\\hat\\beta_1x_i)=0$ can then be rewritten as  \n",
    "\n",
    "\\begin{equation*}\n",
    "\\bar{y}=\\hat\\beta_0+\\hat\\beta_1\\bar{x}\n",
    "\\end{equation*}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat\\beta_0=\\bar{y}-\\hat\\beta_1\\bar{x}\n",
    "\\end{equation*}\n",
    "\n",
    "- Once we have the slope estimate $\\beta_1$, it is straightforward to obtain the intercept estimate $\\beta_0$, given $y$ and $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Derivation of the OLS Estimator (Self Study)**\n",
    "\n",
    "- Plugging the last equation into $n^{-1}\\sum_{i=1}^nx_i(y_i-\\hat\\beta_0-\\hat\\beta_1x_i)=0$ and dropping the $n^{-1}$ (because it does not affect the solution) yields  \n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{i=1}^nx_i[y_i-(\\bar{y}-\\hat\\beta_1\\bar{x})-\\hat\\beta_1x_i)]=0\n",
    "\\end{equation*}\n",
    "\n",
    " which can be rearranged to  \n",
    " \n",
    " \\begin{equation*}\n",
    "\\sum_{i=1}^nx_i(y_i-\\bar{y})=\\hat\\beta_1\\sum_{i=1}^nx_i(x_i-\\bar{x})\n",
    "\\end{equation*}\n",
    "\n",
    "- And because\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{i=1}^nx_i(x_i-\\bar{x})=\\sum_{i=1}^n(x_i-\\bar{x})^2\n",
    "\\end{equation*}\n",
    "\n",
    "and \n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{i=1}^nx_i(y_i-\\bar{y})=\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Derivation of the OLS Estimator (Self Study)**\n",
    "\n",
    "- The estimated slope therefore is  \n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\beta_1}=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\n",
    "\\end{equation*}\n",
    "\n",
    " - This is simply the sample covariance between $x_i$ and $y_i$ divided by the sample variance of $x_i$.\n",
    " \n",
    " - Or:\n",
    " \n",
    "\\begin{equation*}\n",
    "\\hat{\\beta}_{1}=\\frac{Cov[y,x_1]}{Var[x_1]}\n",
    "\\end{equation*}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\beta}_{0}=\\bar{y}-\\hat{\\beta}_{1}\\bar{x_1}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Derivation of the OLS Estimator (Self Study)**\n",
    "\n",
    "- $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ are called the **ordinary least squares** estimators of the parameters.\n",
    "- The name can be justified as follows:\n",
    "- For any $\\beta_0$ and $\\beta_1$ define a **fitted value** for $y$ where $x$ = $x_i$ as  \n",
    "    \n",
    "\\begin{equation*}\n",
    "\\hat{y}_i=\\hat{\\beta}_0+\\hat{\\beta}_1x_i\n",
    "\\end{equation*}\n",
    "\n",
    "- This is the value we predict for $y$ when $x=x_i$ for the given intercept and slope.\n",
    "- There is a fitted value for each observation in the sample.\n",
    "- The **residual** for observation $i$ is the difference between the actual $y_i$ and its fitted value:\n",
    "    \n",
    "\\begin{equation*}\n",
    "\\hat{u}_i=y_i-\\hat{y}_i=y_i-\\hat{\\beta}_0+\\hat{\\beta}_1x_i\n",
    "\\end{equation*}\n",
    "\n",
    "-  - Now, suppose we choose $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ to make the **sum of squared residuals**,\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{i=1}^n\\hat{u}_i^2=\\sum_{i=1}^n(y_i-\\hat{\\beta}_0+\\hat{\\beta}_1x_i)^2\n",
    "\\end{equation*}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Derivation of the OLS Estimator (Self Study)**\n",
    "\n",
    "- The conditions necessary to minimize above equation (first order conditions) are given exactly by the two equations we introduced earlier:\n",
    "\\begin{equation*}\n",
    "n^{-1}\\sum_{i=1}^n(y_i-\\hat\\beta_0-\\hat\\beta_1x_i)=0\n",
    "\\end{equation*}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation*}\n",
    "n^{-1}\\sum_{i=1}^nx_i(y_i-\\hat\\beta_0-\\hat\\beta_1x_i)=0\n",
    "\\end{equation*}\n",
    "\n",
    "- From our previous calculations we know that the solutions ot the OLS first order conditions are given by\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\beta}_{1}=\\frac{Cov[y,x_1]}{Var[x_1]}\n",
    "\\end{equation*}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\beta}_{0}=\\bar{y}-\\hat{\\beta}_{1}\\bar{x_1}\n",
    "\\end{equation*}\n",
    "\n",
    "- The name “ordinary least squares” comes from the fact that these estimates minimize the sum of squared residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- When we view ordinary least squares as minimizing the sum of squared residuals, the question is obvious: Why not minimize some other function of the residuals, such as the absolute values of the residuals?\n",
    "- First, we cannot obtain formulas for the resulting estimators; given a data set, the estimates must be obtained by numerical optimization routines.\n",
    "- As a consequence, the statistical theory for estimators that minimize the sum of the absolute residuals is very complicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Fitted Values and Residuals**\n",
    "\n",
    "<br>  \n",
    "\n",
    "\n",
    "<center><img src=\"figs/wool_2_4.png\" width=\"500\"/> \n",
    "    \n",
    "\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Simple Regression Model**\n",
    "\n",
    "- We are concerned with estimating the population parameters $\\beta_0$ and $\\beta_1$, of the simple linear regression model  \n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "y=\\beta_0+\\beta_1x+u\n",
    "\\end{equation*}\n",
    "\n",
    "   from a random sample of $y$ and $x$. \n",
    "- The ordinary least squares (OLS) estimators are  \n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\beta_0}=\\bar{y}-\\hat{\\beta_1}\\bar{x}  \n",
    "\\end{equation*} \n",
    "\n",
    "\\begin{equation*} \n",
    "\\hat{\\beta_1}=\\frac{Cov(x,y)}{Var(x)}\n",
    "\\end{equation*}  \n",
    "\n",
    "\n",
    "- Based on these estimated parameters, the OLS regression line is  \n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{y}=\\hat{\\beta_0}+\\hat{\\beta_1}x\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "- For a given sample, we just need to calculate the four statistics $\\bar{y},\\bar{x}, Cov(x,y)$ and $Var(x)$ and plug them into these equations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Some examples of simple regressions obtained by using real data**\n",
    "\n",
    "- We now show some examples of simple regressions\n",
    "- Note: \n",
    "    - be careful not to read too much into these regressions; they are not necessarily uncovering a causal relationship.\n",
    "    - We have said nothing so far about the statistical properties of OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example: CEO Salary and Return on Equity**\n",
    "\n",
    "<blockquote> For the population of chief executive officers, \n",
    "    \n",
    "- let $y$ be annual salary ($salary$) in thousands of dollars.\n",
    "    - Thus, $y$=856.3 indicates an annual salary of \\$856,300,\n",
    "    - and $y$=1,452.6 indicates a salary of \\$1,452,600.\n",
    "\n",
    "- Let $x$ be the average return on equity ($roe$) for the CEO’s firm for the previous three years.\n",
    "    - Return on equity is defined in terms of net income as a percentage of common equity.)\n",
    "    - For example, if $roe$=10, then average return on equity is 10%.\n",
    "    \n",
    "To study the relationship between this measure of firm  performance and CEO compensation, we postulate the simple model  \n",
    "    \n",
    "\\begin{equation*} \n",
    "salary = \\beta_0 +\\beta_1roe+u\n",
    "\\end{equation*}  \n",
    "\n",
    "How do you interpret this simple model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model Interpretation\n",
    "\n",
    "- The constant  $\\beta_0$  tells us about the effect when  $roe$  is zero (average baseline salary).\n",
    "- The slope parameter $\\beta_1$ measures the change in annual salary, in thousands of dollars, when return on equity increases by one percentage point.\n",
    "- Because a higher $roe$ is good for the company, we think $\\beta_1>$0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example: CEO Salary and Return on Equity**\n",
    "\n",
    "We are using the data set CEOSAL1. \n",
    "- information on 209 CEOs for the year 1990; these data were obtained from *Business Week* (5/6/91). \n",
    "- In this sample, the average annual salary is $\\$1,281,120$, with the smallest and largest being $\\$223,000$ and $\\$14,822,000$, respectively. \n",
    "- The average return on equity for the years 1988, 1989, and 1990 is $17.18\\%$, with the smallest and largest values being $0.5\\%$ and $56.3\\%$, respectively.\n",
    "\n",
    "We consider the simple regression\n",
    "model  \n",
    "\n",
    "\n",
    "\\begin{equation*} \n",
    "salary = \\beta_0 +\\beta_1roe+u\n",
    "\\end{equation*}  \n",
    "\n",
    "where $salary$ is the salary of a CEO in thousand dollars and $roe$ is the return on investment in percent.\n",
    "\n",
    "- We first load the modules and the data set.\n",
    "- We also calculate the four statistics we need so we can reproduce the OLS formulas by hand.\n",
    "- Finally, the parameter estimates are calculated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1: 18.50118634521492\n",
      "\n",
      "b0: 963.191336472558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "\n",
    "ceosal1 = woo.dataWoo('ceosal1')\n",
    "x = ceosal1['roe']\n",
    "y = ceosal1['salary']\n",
    "\n",
    "# ingredients to the OLS formulas:\n",
    "cov_xy = np.cov(x, y)[1, 0]  # access 2. row and 1. column of covariance matrix\n",
    "var_x = np.var(x, ddof=1)\n",
    "x_bar = np.mean(x)\n",
    "y_bar = np.mean(y)\n",
    "\n",
    "# manual calculation of OLS coefficients:\n",
    "b1 = cov_xy / var_x\n",
    "b0 = y_bar - b1 * x_bar\n",
    "print(f'b1: {b1}\\n')\n",
    "print(f'b0: {b0}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<blockquote>So the OLS regression line is  \n",
    "\n",
    "<b>\n",
    "    \n",
    "\\begin{equation*} \n",
    "salary = 963.1913 + 18.50119 \\cdot roe.\n",
    "\\end{equation*}  \n",
    "\n",
    "</b> \n",
    "\n",
    "How do we have to interpret it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model Interpretation\n",
    "\n",
    "Intercept $\\beta_0$\n",
    "- if the return on equity is zero, $roe=$0, then the predicted salary is the intercept, $963.191$, which equals $\\$963,191$ because $salary$ is measured in thousands. \n",
    "\n",
    "Slope coefficient $\\beta_1$\n",
    "- We can write the predicted change in $salary$ as a function of the change in $roe$: $\\hat{\\Delta{salary}}=18.501 \\hat{\\Delta{roe}}$. \n",
    "- This means that if the return on equity increases by one percentage point, $\\Delta{roe}=1$, then salary is predicted to change by about $18.5$, or $\\$18,500$. \n",
    "\n",
    "Because we have a linear equation, this is the estimated change regardless of the initial salary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can easily use the equation to compare predicted salaries at different values of $roe$. \n",
    "- Suppose $roe$=30. Then $salary$=963.191+18.501(30)=1,518,221, which is just over \\$1.5 million. \n",
    "    - However, this does not mean that a particular CEO whose firm had a $roe$=30 earns \\$1,518,221.\n",
    "    - It is the average effect, averaged over the cross section of all CEOs in the sample\n",
    "    - Many other factors affect salary. This is just our prediction from the OLS regression line above. \n",
    "\n",
    "The estimated line is graphed in the next figure, along with the true population regression function $E(salary|roe)$.\n",
    "- We will never know the PRF, so we cannot tell how close the SRF is to the PRF. \n",
    "- Another sample of data will give a different regression line, which may or may not be closer to the population regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The OLS regression line $\\hat{salary}=963.191 + 18.501 roe$ and the (unknown) population regression function**  \n",
    "\n",
    "\n",
    "<br>  \n",
    "\n",
    "\n",
    "<center><img src=\"figs/wool_2_5.png\" width=\"500\"/> \n",
    "    \n",
    "\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Simple Regression Model**\n",
    "\n",
    "While calculating OLS coefficients using this pedestrian approach is straightforward, there is a more convenient way to do it.\n",
    "\n",
    "- Given the importance of OLS regression, it is not surprising that many Python modules have a specialized command to do the calculations automatically.\n",
    "- In the following chapters, we will often use the module `statsmodels` to apply linear regression and other econometric methods.\n",
    "- When working with statsmodels, the first line of code often is:  \n",
    "\n",
    "```python\n",
    "import statsmodels.formula.api as smf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Simple Regression Model**\n",
    "\n",
    "- If the data frame sample contains the values of the dependent variable in column y and those of the regressor in the column $x$, we can calculate the OLS coefficients as\n",
    "\n",
    "```python\n",
    "reg = smf.ols(formula='y ~ x', data=sample)\n",
    "results = reg.fit()\n",
    "```\n",
    "\n",
    "- The first argument $y$ ~ $x$ is called a `formula`.\n",
    "    - Means that we want to model a left-hand-side variable $y$ to be explained by a right-hand-side variable $x$ in a linear fashion. \n",
    "- Second line of code: calculation of OLS coefficients and many other results are performed by calling the method `fit`.\n",
    "- Finally, all kind of results are assigned to the variable `results'.\n",
    "\n",
    "- We can access both estimated parameters with `results.params`: $\\beta_0$ is labeled Intercept and $\\beta_1$ is labeled with the name of the explanatory variable $roe$.\n",
    "- The values are the same we already calculated except for different rounding in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b: \n",
      "Intercept    963.191336\n",
      "roe           18.501186\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wooldridge as woo\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "ceosal1 = woo.dataWoo('ceosal1')\n",
    "\n",
    "reg = smf.ols(formula='salary ~ roe', data=ceosal1)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "print(f'b: \\n{b}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***Graphical representation of the regression line through observations***\n",
    "\n",
    "- Given the results from a regression, plotting the regression line is straightforward.\n",
    "- The command `plot` can add points to a graph.\n",
    "- In this case, we simply supply the regressor $roe$ and the predicted values (available under `results.fittedvalues`) and connect them by a line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'roe')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wooldridge as woo\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ceosal1 = woo.dataWoo('ceosal1')\n",
    "\n",
    "# OLS regression:\n",
    "reg = smf.ols(formula='salary ~ roe', data=ceosal1)\n",
    "results = reg.fit()\n",
    "\n",
    "# scatter plot and fitted values:\n",
    "plt.plot('roe', 'salary', data=ceosal1, color='grey', marker='o', linestyle='')\n",
    "plt.plot(ceosal1['roe'], results.fittedvalues, color='black', linestyle='-')\n",
    "plt.ylabel('salary')\n",
    "plt.xlabel('roe')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Coefficients, Fitted Values, and Residuals**\n",
    "\n",
    "- The object returned by the method `fit` contains all relevant information on the regression.\n",
    "- After defining the regression results object `results` in CEO script, we can access the OLS coefficients with  \n",
    "\n",
    "```python\n",
    "results.params\n",
    "```\n",
    "\n",
    "- The coefficient object has names attached to its elements.\n",
    "- The name of the intercept parameter $\\beta_0$ is `Intercept` and the name of the slope parameter $\\beta_1$ is the variable name of the regressor $x$.\n",
    "- In this way, we can access the parameters separately by using either the position (0 or 1) or the name as an index to the coefficients object.\n",
    "- For example, in CEO example you can access intercept and slope parameter as below:  \n",
    "\n",
    "```python\n",
    "b[0] # intercept\n",
    "b['roe'] # slope parameter\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Given these parameter estimates, calculating the predicted values $\\hat{y_i}$ and residuals $\\hat{u_i}$ for each observation $i = 1,...,n$ is easy:  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y_i}=\\hat{\\beta_0}+\\hat{\\beta_1}\\cdot x_i\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{u_i}=y_i+\\hat{y_i}\n",
    "\\end{equation}\n",
    "\n",
    "- If the values of the dependent and independent variables are stored in a data frame `sample` as $y$ and $x$, respectively, we can estimate the model and do the calculations of these equations for all observations jointly using the code\n",
    "\n",
    "```python\n",
    "reg = smf.ols(formula='y ~ x', data=sample)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "y_hat = b[0] + b[1] * sample['x']\n",
    "u_hat = sample['y'] - y_hat\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example: CEO Salary and Return on Equity**\n",
    "\n",
    "We extend the earlier regression example on the return on equity of a firm and the salary of its CEO.\n",
    "- After the OLS regression, we calculate fitted values and residuals.\n",
    "- A table is generated displaying the values for the first 15 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table.head(15): \n",
      "          roe  salary  salary_hat1      u_hat1\n",
      "0   14.100000    1095  1224.058071 -129.058071\n",
      "1   10.900000    1001  1164.854261 -163.854261\n",
      "2   23.500000    1122  1397.969216 -275.969216\n",
      "3    5.900000     578  1072.348338 -494.348338\n",
      "4   13.800000    1368  1218.507712  149.492288\n",
      "5   20.000000    1145  1333.215063 -188.215063\n",
      "6   16.400000    1078  1266.610785 -188.610785\n",
      "7   16.299999    1094  1264.760660 -170.760660\n",
      "8   10.500000    1237  1157.453793   79.546207\n",
      "9   26.299999     833  1449.772523 -616.772523\n",
      "10  25.900000     567  1442.372056 -875.372056\n",
      "11  26.799999     933  1459.023116 -526.023116\n",
      "12  14.800000    1339  1237.008898  101.991102\n",
      "13  22.299999     937  1375.767778 -438.767778\n",
      "14  56.299999    2011  2004.808114    6.191886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wooldridge as woo\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "ceosal1 = woo.dataWoo('ceosal1')\n",
    "\n",
    "# OLS regression:\n",
    "reg = smf.ols(formula='salary ~ roe', data=ceosal1)\n",
    "results = reg.fit()\n",
    "\n",
    "# obtain predicted values and residuals by hand:\n",
    "b = results.params\n",
    "salary_hat1 = b[0] + b[1] * ceosal1['roe']\n",
    "\n",
    "# You can also write it like that:\n",
    "#salary_hat1 = b['Intercept'] + b['roe'] * ceosal1['roe']\n",
    "\n",
    "u_hat1 = ceosal1['salary'] - salary_hat1\n",
    "\n",
    "\n",
    "# Output\n",
    "table = pd.DataFrame({'roe': ceosal1['roe'],\n",
    "                      'salary': ceosal1['salary'],\n",
    "                      'salary_hat1': salary_hat1,\n",
    "                      'u_hat1': u_hat1})\n",
    "print(f'table.head(15): \\n{table.head(15)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also use a more black-box approach which will give exactly the same results using the\n",
    "precalculated variables `fittedvalues` and `resid` on the regression results object:  \n",
    "\n",
    "```python\n",
    "reg = smf.ols(formula='y ~ x', data=sample)\n",
    "results = reg.fit()\n",
    "y_hat = results.fittedvalues\n",
    "u_hat = results.resid\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table.head(15): \n",
      "          roe  salary  salary_hat1      u_hat1  salary_hat2      u_hat2\n",
      "0   14.100000    1095  1224.058071 -129.058071  1224.058071 -129.058071\n",
      "1   10.900000    1001  1164.854261 -163.854261  1164.854261 -163.854261\n",
      "2   23.500000    1122  1397.969216 -275.969216  1397.969216 -275.969216\n",
      "3    5.900000     578  1072.348338 -494.348338  1072.348338 -494.348338\n",
      "4   13.800000    1368  1218.507712  149.492288  1218.507712  149.492288\n",
      "5   20.000000    1145  1333.215063 -188.215063  1333.215063 -188.215063\n",
      "6   16.400000    1078  1266.610785 -188.610785  1266.610785 -188.610785\n",
      "7   16.299999    1094  1264.760660 -170.760660  1264.760660 -170.760660\n",
      "8   10.500000    1237  1157.453793   79.546207  1157.453793   79.546207\n",
      "9   26.299999     833  1449.772523 -616.772523  1449.772523 -616.772523\n",
      "10  25.900000     567  1442.372056 -875.372056  1442.372056 -875.372056\n",
      "11  26.799999     933  1459.023116 -526.023116  1459.023116 -526.023116\n",
      "12  14.800000    1339  1237.008898  101.991102  1237.008898  101.991102\n",
      "13  22.299999     937  1375.767778 -438.767778  1375.767778 -438.767778\n",
      "14  56.299999    2011  2004.808114    6.191886  2004.808114    6.191886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# obtain predicted values and residuals using fittedvalues and resid:\n",
    "salary_hat2 = results.fittedvalues\n",
    "u_hat2 = results.resid\n",
    "\n",
    "# Output with comparison:\n",
    "table = pd.DataFrame({'roe': ceosal1['roe'],\n",
    "                      'salary': ceosal1['salary'],\n",
    "                      'salary_hat1': salary_hat1,\n",
    "                      'u_hat1': u_hat1,\n",
    "                      'salary_hat2': salary_hat2,\n",
    "                      'u_hat2': u_hat2})\n",
    "print(f'table.head(15): \\n{table.head(15)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also easily confirm the three properties of OLS introduced before:\n",
    "\n",
    "- The sum, and therefore the sample average of the OLS residuals, is zero.\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^n\\hat{u}_i=0 \\rightarrow \\bar{u}_i=0\n",
    "\\end{equation}\n",
    "\n",
    "- The Covariance of $x$ and $u$ is zero\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^nx_i\\hat{u}_i=0 \\rightarrow Cov(x_i,\\hat{u}_i)=0\n",
    "\\end{equation}\n",
    "\n",
    "- The point $(\\bar{x},\\bar{y})$ is always on the OLS regression line.\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{y}=\\hat{\\beta}_0+\\hat{\\beta}_1\\cdot \\bar{x}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_hat_mean: -1.109670569148788e-13\n",
      "\n",
      "roe_u_cov: -8.395335708673184e-13\n",
      "\n",
      "salary_pred: 1281.1196172248806\n",
      "\n",
      "salary_mean: 1281.1196172248804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "ceosal1 = woo.dataWoo('ceosal1')\n",
    "reg = smf.ols(formula='salary ~ roe', data=ceosal1)\n",
    "results = reg.fit()\n",
    "\n",
    "# obtain coefficients, predicted values and residuals:\n",
    "b = results.params\n",
    "salary_hat = results.fittedvalues\n",
    "u_hat = results.resid\n",
    "\n",
    "# confirm property (1):\n",
    "u_hat_mean = np.mean(u_hat)\n",
    "print(f'u_hat_mean: {u_hat_mean}\\n')\n",
    "\n",
    "# confirm property (2):\n",
    "roe_u_cov = np.cov(ceosal1['roe'], u_hat) [1, 0] #access 2. row and 1. column of covariance matrix\n",
    "print(f'roe_u_cov: {roe_u_cov}\\n')\n",
    "\n",
    "# confirm property (3):\n",
    "roe_mean = np.mean(ceosal1['roe'])\n",
    "salary_pred = b[0] + b[1] * roe_mean\n",
    "print(f'salary_pred: {salary_pred}\\n')\n",
    "\n",
    "salary_mean = np.mean(ceosal1['salary'])\n",
    "print(f'salary_mean: {salary_mean}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Goodness of Fit**\n",
    "\n",
    "We now derive a method that allows to say sth about the (relative) quality of our regression.\n",
    "\n",
    "As said before, OLS decomposes each $y_i$ into two parts, \n",
    "- (i) a fitted value and (ii) a residual\n",
    "- The fitted value and the residual are uncorrelated in the sample\n",
    "\n",
    "We define the **total sum of squares (SST)**, the **explained sum of squares (SSE)**, and the **residual sum of squares (SSR)**\n",
    "- The total variation of $y$ is the sum of the explained variation and the unexplained variation SSR.\n",
    "\n",
    "<center>$SST=SSE+SSR$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Goodness of Fit**\n",
    "\n",
    "**SST** is a measure of the total sample variation in the $y_i$; that is, it measures how spread out the $y_i$ are in the sample.\n",
    "\n",
    "\\begin{equation}\n",
    "SST=\\sum_{i=1}^n(y_i-\\bar{y})^2=(n-1)\\cdot Var({y})\n",
    "\\end{equation}  \n",
    "  - If we divide SST by $n-1$, we obtain the sample variance of $y$.\n",
    "\n",
    "**SSE** measures the variation of $\\hat{y}_i$ (where we use the fact that $\\bar{\\hat{y}}=\\bar{y}$), \n",
    "\n",
    "\\begin{equation}\n",
    "SSE=\\sum_{i=1}^n(\\hat{y}_i-\\bar{y})^2=(n-1)\\cdot Var(\\hat{y})\n",
    "\\end{equation}\n",
    "\n",
    "**SSR** measures the variation of $y_i$ that can be explained by the $u_i$ (not by $x_i$).\n",
    "\\begin{equation}\n",
    "SSR=\\sum_{i=1}^n({\\hat{u}_i})^2=(n-1)\\cdot Var(\\hat{u})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Goodness of Fit**\n",
    "\n",
    "Using the above sample variation decomposition, we can compute the $R^2$ statistic that summarizes how well the OLS regression line fits the data\n",
    "- $R^2$ is the ratio of the explained variation compared to the total variation\n",
    "- It is interpreted as the fraction of the sample variation in $y$ that is explained by $x$.\n",
    "\n",
    "\\begin{equation}\n",
    "R^2=SSE/SST=1-SSR/SST\n",
    "\\end{equation}\n",
    "\n",
    "- The value of $R^2$ is always between zero and one, because SSE can be no greater than SST.\n",
    "- When interpreting $R^2$, we usually multiply it by 100 to change it into a percent: \n",
    "\n",
    "100 $\\cdot R^2$ is the percentage of the sample variation in $y$ that is explained by $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example: CEO Salary and Return on Equity**\n",
    "\n",
    "<blockquote> In the regression already studied before, the $R^2$ is 0.0132. This is calculated in the two ways outlined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2_a: 0.01318862408103412\n",
      "\n",
      "R2_b: 0.01318862408103405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "ceosal1 = woo.dataWoo('ceosal1')\n",
    "\n",
    "# OLS regression:\n",
    "reg = smf.ols(formula='salary ~ roe', data=ceosal1)\n",
    "results = reg.fit()\n",
    "\n",
    "# calculate predicted values & residuals:\n",
    "sal_hat = results.fittedvalues\n",
    "u_hat = results.resid\n",
    "\n",
    "# calculate R^2 in three different ways:\n",
    "sal = ceosal1['salary']\n",
    "R2_a = np.var(sal_hat, ddof=1) / np.var(sal, ddof=1)\n",
    "R2_b = 1 - np.var(u_hat, ddof=1) / np.var(sal, ddof=1)\n",
    "\n",
    "print(f'R2_a: {R2_a}\\n')\n",
    "print(f'R2_b: {R2_b}\\n')\n",
    "\n",
    "#ceosal1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Goodness of Fit**\n",
    "\n",
    "- In the social sciences, low *R-squared*s in regression equations are not uncommon, especially for cross-sectional analysis.\n",
    "- We will discuss this issue more generally under multiple regression analysis, but it is worth emphasizing now that a seemingly low R-squared does not necessarily mean that an OLS regression equation is useless.\n",
    "- It is still possible that we have a good estimate of the ceteris paribus relationship between *salary* and *roe*; whether or not this is true does not depend directly on the size of R-squared.\n",
    "- Students who are first learning econometrics tend to put too much weight on the size of the R-squared in evaluating regression equations.\n",
    "- For now, be aware that using R-squared as the main gauge of success for an econometric analysis can lead to trouble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Goodness of Fit**\n",
    "\n",
    "- Many interesting results for a regression can be displayed by calling the method `summary`.\n",
    "- You call this method on the object returned by the method `fit`\n",
    "- The output will display\n",
    "    - a block of general information about the regression model. It contains also other information about the estimation of which only $R^2$ is of interest to us so far. It is reported as **R-squared**.\n",
    "    - A coefficient table. So far, we only discussed the OLS coefficients shown in the first column. The next columns will be introduced below.\n",
    "    - A block of diagnostics regarding the residuals. We will discuss some of them later. When we are only interested in the coefficients and their significance, we will often switch to a more compact presentation of results.This is demonstrated with the object `table` in the following example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example: Voting Outcomes and Campaign Expenditures**\n",
    "\n",
    "<blockquote>\n",
    "    \n",
    "- The file VOTE1 contains data on election outcomes and campaign expenditures for 173 two-party races for the U.S. House of Representatives in 1988.\n",
    "- There are two candidates in each race, A and B.\n",
    "- Let voteA be the percentage of the vote received by Candidate A and shareA be the percentage of total campaign expenditures accounted for by Candidate A.\n",
    "- Many factors other than shareA affect the election outcome (including the quality of the candidates and possibly the dollar amounts spent by A and B).\n",
    "- Nevertheless, we can estimate a simple regression model to find out whether spending more relative to one's challenger implies a higher percentage of the vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results.summary(): \n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  voteA   R-squared:                       0.856\n",
      "Model:                            OLS   Adj. R-squared:                  0.855\n",
      "Method:                 Least Squares   F-statistic:                     1018.\n",
      "Date:                Sun, 12 Sep 2021   Prob (F-statistic):           6.63e-74\n",
      "Time:                        21:55:49   Log-Likelihood:                -565.20\n",
      "No. Observations:                 173   AIC:                             1134.\n",
      "Df Residuals:                     171   BIC:                             1141.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     26.8122      0.887     30.221      0.000      25.061      28.564\n",
      "shareA         0.4638      0.015     31.901      0.000       0.435       0.493\n",
      "==============================================================================\n",
      "Omnibus:                       20.747   Durbin-Watson:                   1.826\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               44.613\n",
      "Skew:                           0.525   Prob(JB):                     2.05e-10\n",
      "Kurtosis:                       5.255   Cond. No.                         112.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "table: \n",
      "                 b      se        t  pval\n",
      "Intercept  26.8122  0.8872  30.2207   0.0\n",
      "shareA      0.4638  0.0145  31.9008   0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wooldridge as woo\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "\n",
    "vote1 = woo.dataWoo('vote1')\n",
    "\n",
    "# OLS regression:\n",
    "reg = smf.ols(formula='voteA ~ shareA', data=vote1)\n",
    "results = reg.fit()\n",
    "\n",
    "# print results using summary:\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n",
    "\n",
    "# print regression table:\n",
    "table = pd.DataFrame({'b': round(results.params, 4),\n",
    "                      'se': round(results.bse, 4),\n",
    "                      't': round(results.tvalues, 4),\n",
    "                      'pval': round(results.pvalues, 4)})\n",
    "print(f'table: \\n{table}\\n')\n",
    "#vote1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Unit of Measurement and Functional Forms</h2>\n",
    "\n",
    "Two important issues in applied economics are\n",
    "1. understanding how changing the units of measurement of the dependent and/or independent variables affects OLS estimates and\n",
    "2. knowing how to incorporate popular functional forms used in economics into regression analysis.\n",
    "\n",
    "We now address both issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Effects of Changing Units of Measurement\n",
    "on OLS Statistics** \n",
    "\n",
    "In the _CEO salary_ example we chose to measure annual salary in thousands of dollars, and the return on equity was measured as a percentage (rather than as a decimal).\n",
    "- It is crucial to know how _salary_ and _roe_ are measured in this example in order to make sense of the parameter estimates\n",
    "- We must also know that OLS estimates change in entirely expected ways when the units of measurement of the dependent and independent variables change.\n",
    "\n",
    "In the CEO example, suppose that, rather than measuring salary in thousands of dollars, we measure it in dollars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results.summary(): \n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 salary   R-squared:                       0.013\n",
      "Model:                            OLS   Adj. R-squared:                  0.008\n",
      "Method:                 Least Squares   F-statistic:                     2.767\n",
      "Date:                Tue, 15 Sep 2020   Prob (F-statistic):             0.0978\n",
      "Time:                        10:24:03   Log-Likelihood:                -3248.3\n",
      "No. Observations:                 209   AIC:                             6501.\n",
      "Df Residuals:                     207   BIC:                             6507.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   9.632e+05   2.13e+05      4.517      0.000    5.43e+05    1.38e+06\n",
      "roe          1.85e+04   1.11e+04      1.663      0.098   -3428.196    4.04e+04\n",
      "==============================================================================\n",
      "Omnibus:                      311.096   Durbin-Watson:                   2.105\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            31120.902\n",
      "Skew:                           6.915   Prob(JB):                         0.00\n",
      "Kurtosis:                      61.158   Cond. No.                         43.3\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wooldridge as woo\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "ceosal1 = woo.dataWoo('ceosal1')\n",
    "ceosal1['salary'] = ceosal1['salary']*1000\n",
    "# You could also replace the original 'salary' variable with the same command. Be careful about that!\n",
    "\n",
    "\n",
    "# OLS regression:\n",
    "\n",
    "reg = smf.ols(formula='salary ~ roe', data=ceosal1)\n",
    "#reg = smf.ols(formula='salary2 ~ roe', data=ceosal1)\n",
    "\n",
    "results = reg.fit()\n",
    "\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Incorporating Nonlinearities in Simple Regression**\n",
    "\n",
    "So far, we have focused on linear relationships between the dependent and independent variables.\n",
    "- But: linear relationships are not nearly general enough for all economic applications.\n",
    "- But: also easy to incorporate many nonlinearities into simple regression analysis by appropriately defining the dependent and independent variables.\n",
    "\n",
    "Here, we will cover two possibilities that often appear in applied work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Incorporating Nonlinearities in Simple Regression**\n",
    "\n",
    "In reading applied work in economics, you will often encounter regression equations where the dependent variable appears in **logarithmic form**.\n",
    "\n",
    "Why is the logarithmic transformation attractive?\n",
    "- Recall the wage-education example, where we regressed hourly wage on years of education.\n",
    "- We obtained a slope estimate of 0.54, which means that each additional year of education is predicted to increase hourly wage by 54 cents.\n",
    "- Because of the linear nature of this model, 54 cents is the increase for either the first year of education or the twentieth year; this may not be reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Incorporating Nonlinearities in Simple Regression**\n",
    "\n",
    "Probably a better characterization of how wage changes with education is that each year of education increases wage by a constant percentage.\n",
    "\n",
    "- For example, an increase in education from 5 years to 6 years increases wage by, say, 8% (ceteris paribus), and an increase in education from 11 to 12 years also increases wage by 8%.\n",
    "- A model that gives (approximately) a constant percentage effect is\n",
    "    \n",
    "    \\begin{equation}\n",
    "    log(wage)=\\beta_0+\\beta_1educ+u\n",
    "    \\end{equation}  \n",
    "    \n",
    "    \n",
    "where $log(wage)$ is the _natural_ logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Incorporating Nonlinearities in Simple Regression**\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\%\\Delta{wage} \\approx (100\\cdot\\beta_1)\\Delta{educ}\n",
    "\\end{equation}\n",
    "    \n",
    "- Notice how we multiply $\\beta_1$ by 100 to get the percentage change in wage given one additional year of education.\n",
    "- Because the percentage change in wage is the same for each additional year of education, the change in wage for an extra year of education increases as education increases; in other words, it implies an increasing return to education.\n",
    "- By exponentiating the above equation, we can write $wage=exp(\\beta_0+\\beta_1educ+u)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$wage=exp(\\beta_0+\\beta_1educ)$, with $\\beta_1>0$\n",
    "\n",
    "\n",
    "<br>  \n",
    "\n",
    "\n",
    "<center><img src=\"figs/wool_2_6.png\" width=\"500\"/> \n",
    "    \n",
    "\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example: A Log Wage Equation**\n",
    "\n",
    "<blockquote> Using the same data as in the wage and education example, but using log(wage)_as the dependent variable, we obtain the following relationship:\n",
    "    \n",
    "<br>\n",
    "    \n",
    "    \n",
    "\\begin{equation}\n",
    "log(wage)=\\beta_0+\\beta_1educ+u\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b: \n",
      "Intercept    0.583773\n",
      "educ         0.082744\n",
      "dtype: float64\n",
      "\n",
      "r2: \n",
      " 0.18580647728823507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import wooldridge as woo\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "wage1 = woo.dataWoo('wage1')\n",
    "\n",
    "# estimate log-level model:\n",
    "reg = smf.ols(formula='np.log(wage) ~ educ', data=wage1)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "print(f'b: \\n{b}\\n')\n",
    "print(f'r2: \\n {results.rsquared}\\n')\n",
    "#print(f'results.summary(): \\n{results.summary()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The coefficient on educ has a percentage interpretation when it is multiplied by 100: wage increases by 8.3% for every additional year of education.\n",
    "- It is important to remember that the main reason for using the log of wage here is to impose a constant percentage effect of education on wage.\n",
    "- Once the above log equation is obtained, the natural log of wage is rarely mentioned.\n",
    "- In particular, it is not correct to say that another year of education increases _log(wage)_ by 8.3%. \n",
    "- The intercept is not very meaningful, because it gives the predicted log(wage), when _educ=0_.\n",
    "- The R-squared shows that educ explains about 18.6% of the variation in log(wage) (not wage).\n",
    "\n",
    "Are there potentially additional non-linearities that are not captured by the log(wage) equation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Indeed, the equation might not capture all of the nonlinearity in the relationship between wage and schooling.\n",
    "\n",
    "- If there are “diploma effects,” then the twelfth year of education—graduation from high school—could be worth much more than the eleventh year.\n",
    "- We will learn how to allow for this kind of nonlinearity in later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another important use of the natural log is in obtaining a **constant elasticity model**:\n",
    "\n",
    "- Elasticity: \\% change of $y$ after 1\\% increase of $x$ \n",
    "\n",
    "We can estimate a constant elasticity model relating **CEO salary to firm sales**.\n",
    "- The data set is the same as before, except we now relate _salary_ to _sales_.\n",
    "- Let sales be annual firm sales, measured in millions of dollars. \n",
    "\n",
    "A constant elasticity model is \n",
    "    \n",
    "\\begin{equation}\n",
    "log(salary)=\\beta_0+\\beta_1 log(sales)+u\n",
    "\\end{equation}\n",
    "\n",
    "where $\\beta_1$ is the elasticity of salary with respect to _sales_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Estimating this equation by OLS gives\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{log(salary)}=4.822+0.257 log(sales)+u\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "n=209, R^2=0.211\n",
    "\\end{equation}\n",
    "\n",
    "- The coefficient of log(*sales*) is the estimated elasticity of *salary* with respect to *sales*.\n",
    "- It implies that a 1% increase in firm sales increases CEO salary by about 0.257% — the usual interpretation of an elasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Summary of Functional Forms Involving Logarithms**\n",
    "\n",
    "\n",
    "| Model | Dependent Variable | Independent Variable | Interpretation of $\\beta_1$  &nbsp; &nbsp; &nbsp; &nbsp; |\n",
    "| - | - | - | - | \n",
    "| Level-level | y | x | $\\Delta y = \\beta_1\\Delta x$ |\n",
    "| Level-log | y | log(x) | $\\Delta y =(\\beta_1/100)\\%\\Delta x$    |\n",
    "| Log-level |log(y) | x | $\\%\\Delta y = (100\\beta_1)\\Delta x$    |\n",
    "| Log-log | log(y) | log(x) | $\\%\\Delta y = \\beta_1\\%\\Delta x$    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The Meaning of “Linear” Regression**\n",
    "\n",
    "The simple regression model that we have studied in this chapter is also called the simple _linear_ regression model.\n",
    "\n",
    "- Yet, as we have just seen, the general model also allows for certain _nonlinear_ relationships.\n",
    "\n",
    "So what does “linear” mean here? \n",
    "\n",
    "- The key is that this equation is linear in the parameters!\n",
    "- There are no restrictions on how $y$ and $x$ relate to the original explained and explanatory variables of interest.\n",
    "- As we saw $y$ and $x$ can be natural logs of variables, and this is quite common in applications.\n",
    "- Whereas the mechanics of simple regression do not depend on how $y$ and $x$ are defined, the interpretation of the coefficients does depend on their definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<br>  \n",
    "\n",
    "\n",
    "<center><img src=\"figs/curve_fitting.png\" width=\"500\"/> \n",
    "    \n",
    "\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Expected Values and Variances of the OLS Estimator</h2>\n",
    "\n",
    "So far we have defined the population model $y=\\beta_0+\\beta_1x+u$, and we claimed that the key assumption for simple regression analysis to be useful is that the expected value of $u$ given any value of $x$ is zero.\n",
    "\n",
    "- We now return to the population model and study the statistical properties of OLS.\n",
    "- In other words, we now view $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ as estimators for the parameters $\\beta_0$ and $\\beta_1$ that appear in the population model.\n",
    "- This means that we will study properties of the distributions $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ over different random samples from the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Properties of OLS**\n",
    "\n",
    "If five assumptions hold, then the OLS estimator $\\hat{\\beta}_1$ is the **B<span>**est **L<span>**inear **U<span>**nbiased **E<span>**stimator (BLUE)\n",
    "- Estimator: Rule that can be applied to any sample of data to produce an estimate \n",
    "- Linear: OLS is in the class of linear models\n",
    "- Unbiased: If we run OLS with different random samples, the expected value of the estimated parameter equals the true (unknown) population parameter\n",
    "- Best = Most efficient: OLS has the lowest variance of all estimators in the linear class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Unbiasedness of OLS**\n",
    "\n",
    "Five assumptions that give the BLUE property to the OLS estimator: \n",
    "\n",
    "- For future reference, it is useful to number these assumptions using the prefix \"SLR\" for simple linear regression.\n",
    "\n",
    "    - SLR.1: Linear in Parameters: $y=\\beta_0+\\beta_1x+u$\n",
    "    - SLR.2: Random Sampling of $y$ and $x$ from the population\n",
    "    - SLR.3: Variation in the sample values $x_1,...,x_n$\n",
    "    - SLR.4: Zero conditional mean: $E(u|x)=0$\n",
    "    - SRL.5: The error $u$ has the same variance given any value of the explanatory variable (homoscedasticity): $Var(u|x)=\\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Unbiasedness of OLS**\n",
    "\n",
    "We begin by establishing the unbiasedness of OLS under a simple set of assumptions SLR.1-SLR.4\n",
    "- **U** in the BLUE\n",
    "\n",
    "**Bias**: If we run OLS with different random samples, the expected value of the estimated parameter equals the true (unknown) population parameter\n",
    "- $E(\\hat\\beta_1)=\\beta_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Graphical Illustration of Unbiasedness**\n",
    "\n",
    "<br>  \n",
    "\n",
    "\n",
    "<center><img src=\"figs/Bias.png\" width=\"500\"/> \n",
    "    \n",
    "\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Proof: Unbiasedness of OLS (Self Study)**\n",
    "\n",
    "- As we have shown before we can write the slope estimator as\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\beta_1}=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\n",
    "\\end{equation*}\n",
    "\n",
    "- Because we are now interested in the behavior of $\\hat{\\beta_1}$ across all possible samples, $\\hat{\\beta_1}$ is properly viewed as a random variable.\n",
    "- We can rewrite the equation so that:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\beta_1}=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})y_i}{SST_x}=\\hat{\\beta_1}=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(\\beta_0+\\beta_1x_i+u_i)}{SST_x}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "...and after some further rearrangement in the nominator\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{i=1}^n(x_i-\\bar{x})\\beta_0+\\sum_{i=1}^n(x_i-\\bar{x})\\beta_1x_i+\\sum_{i=1}^n(x_i-\\bar{x})u_i=\\beta_0\\sum_{i=1}^n(x_i-\\bar{x})+\\beta_1\\sum_{i=1}^n(x_i-\\bar{x})x_i+\\sum_{i=1}^n(x_i-\\bar{x})u_i\n",
    "\\end{equation*}\n",
    "\n",
    "we get - because $\\sum_{i=1}^n(x_i-\\bar{x})=0$ and $\\sum_{i=1}^n(x_i-\\bar{x})x_i=\\sum_{i=1}^n(x_i-\\bar{x})^2= SST_x$ - a nominator of \n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\beta}_1SST_x+\\sum_{i=1}^n(x_i-\\bar{x})u_i\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Thus, we can write $\\hat{\\beta}_1$ as\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\beta_1}=\\beta_1+\\frac{\\sum_{i=1}^n(x_i-\\bar{x})u_i}{SST_x}=\\beta_1+(1/SST_x)\\sum_{i=1}^n(x_i-\\bar{x})u_i\n",
    "\\end{equation*}\n",
    "\n",
    "- We now see that the estimator $\\hat{\\beta}_1$ equals the populations slope parameter $\\beta_1$ plus a term, that is a linear combination in the errors $[u_1,u_2,...,u_n]$. \n",
    "\n",
    "- Because the expected value of each $u_i$ is zero under the assumptions SLR.2 and SLR.4, we get an unbiased estimator $\\hat{\\beta_1}$ for $\\beta_1$\n",
    "\n",
    "\\begin{equation*}\n",
    "E(\\hat{\\beta_1})=\\beta_1+E[(1/SST_x)\\sum_{i=1}^n(x_i-\\bar{x})u_i] =\\beta_1+(1/SST_x)\\sum_{i=1}^nE[(x_i-\\bar{x})u_i]=\\beta_1+(1/SST_x)\\sum_{i=1}^n(x_i-\\bar{x})E(u_i) =\\beta_1+(1/SST_x)\\sum_{i=1}^n(x_i-\\bar{x})\\cdot 0=\\beta_1\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Variances of the OLS Estimators**\n",
    "\n",
    "We now know that the sampling distribution of $\\hat{\\beta_1}$ is centered about $\\beta_1$ ($\\hat{\\beta_1}$ is unbiased)\n",
    "\n",
    "In addition, it is important to know how far we can expect $\\hat{\\beta_1}$ to be away from $\\beta_1$ on average\n",
    "\n",
    "- Efficiency of the estimator: **B** in the **B**LUE\n",
    "\n",
    "Using assumptions SLR.1-SLR.5 we can show that OLS is the best estimator in the linear class of unbiased estimators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Graphical Illustration of Unbiasedness and Efficiency**\n",
    "\n",
    "<br>  \n",
    "\n",
    "\n",
    "<center><img src=\"figs/BiasVariance.png\" width=\"500\"/> \n",
    "    \n",
    "\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Variances of the OLS Estimators**\n",
    "\n",
    "We now turn to some statistic that measures the spread in the distribution of $\\hat{\\beta_1}$ (and $\\hat{\\beta_0}$) \n",
    "- We could use the variance or its square root, the standard deviation.\n",
    "- The variance of the OLS estimators can be computed under Assumptions SLR.1 through SLR.4.\n",
    "- However, these expressions would be somewhat complicated\n",
    "\n",
    "Instead, use the additional assumption SRL.5 that is traditional for cross-sectional analysis.\n",
    "- This assumption states that the variance of the unobservable, $u$, conditional on $x$, is constant.\n",
    "- This is known as the **homoskedasticity** or \"constant variance\" assumption.\n",
    "\n",
    "SRL.5: The error $u$ has the same variance given any value of the explanatory variable (homoscedasticity):\n",
    "\n",
    "\\begin{equation*}\n",
    "Var(u|x)=\\sigma^2\n",
    "\\end{equation*}\n",
    "- The square root of $\\sigma^2$, $\\sigma$, is the _standard deviation_ of the error.\n",
    "- A larger $\\sigma$ means that the distribution of the unobservables affecting $y$ is more spread out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Variances of the OLS Estimators (Self Study)**\n",
    "\n",
    "NOTE: the homoskedasticity assumption is quite distinct from the zero conditional mean assumption, $E(u|x)=0$.\n",
    "\n",
    "- Assumption SLR.4 involves the _expected value_ of $u$, while Assumption SLR.5 concerns the _variance_ of $u$ (both conditional on $x$).\n",
    "\n",
    "Because the definition of variance is $Var(u|x)=E(u^2|x)-E(u|x)^2$ and we assume that $E(u|x)=0$, it follows $\\sigma^2=E(u^2|x)$,\n",
    "\n",
    "Then by law of iterated expectations, $\\sigma^2$ is also the _unconditional expectation_ of $u^2$ and, therefore, $\\sigma^2=E(u^2)=Var(u)$, because $E(u)=0$\n",
    "\n",
    "- In other words, $\\sigma^2$ is the _unconditional variance_ of $u$, and so $\\sigma^2$ is often called the **error variance** or disturbance variance.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Remember (Self Study contin.): \n",
    "\n",
    "1. The variance is the mean squared deviation from its own mean: $Var(x)= E(X- \\bar{x})^2$ which is by the law of expectation equal to $E(X^2)-E(X)^2$\n",
    "    - $Var(x)= E(X- \\bar{x})^2=E(X^2)-E(X)^2$\n",
    "2. It follows from the law of iterated expectations: the expected value of the conditional expected value of u given X is the same as the expected value of u:\n",
    "    - $E(u^2)=E[E(u^2|x)]=E(\\sigma^2)=E(\\sigma^2)$\n",
    "    - Wooldridge, Introductory Econometrics, Appendix B, Property CE.4, page 687"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Variances of the OLS Estimators**\n",
    "\n",
    "- One can also write assumptions SLR.4 and SLR.5 in terms of conditional means and conditional variance of $y$\n",
    "\n",
    "\\begin{equation*}\n",
    "E(y|x)=\\beta_0+\\beta_1x\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "Var(y|x)=\\sigma^2\n",
    "\\end{equation*}\n",
    "\n",
    "- In other words, the conditional expectation of $y$ given $x$ is linear in $x$, but the variance of $y$ given $x$ is constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example for homoscedastic errors**  \n",
    "\n",
    "\n",
    "\n",
    "<br>  \n",
    "\n",
    "\n",
    "<center><img src=\"figs/wool_2_8.png\" width=\"500\"/> \n",
    "    \n",
    "\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example for homoscedastic errors**  \n",
    "\n",
    "<br>  \n",
    "\n",
    "<center><img src=\"figs/homoscedast.png\" width=\"500\"/> \n",
    "    \n",
    "<p>\n",
    "    \n",
    "The slope of the regression is increasing, but the distribution of error terms around any given value of $x$ is constant. It is therefore easy to see that also the variance of $y$ given $x$ is constant   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Variances of the OLS Estimators**\n",
    "\n",
    "- When $Var(u|x)$ depends on $x$, the error term is said to exhibit **heteroskedasticity** (or nonconstant\n",
    "variance).\n",
    "- Because $Var(u|x)=Var(y|x)$, heteroskedasticity is present whenever $Var(y|x)$ is a function of $x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example for heteroscedastic errors**  \n",
    "\n",
    "<br>  \n",
    "\n",
    "\n",
    "<center><img src=\"figs/heteroscedast.png\" width=\"500\"/> \n",
    "    \n",
    "\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example**: Heteroskedasticity in a Wage Equation   \n",
    "\n",
    "In order to get an unbiased estimator of the ceteris paribus effect of _educ_ on _wage_, we must assume that $E(u|educ)=0$, and this implies $E(wage|educ)=\\beta_0+\\beta_1educ$.\n",
    "\n",
    "\n",
    "- If we also make the homoskedasticity assumption, then $Var (u|educ)=\\sigma^2$ does not depend on the level of education, which is the same as assuming $Var(wage|educ)=\\sigma^2$.\n",
    "- Thus, while average wage is allowed to increase with education level — it is this rate of increase that we are interested in estimating — the variability in wage about its mean is assumed to be constant across all education levels.\n",
    "\n",
    "Realistic?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This may not be realistic:\n",
    "\n",
    " It is likely that people with more education have a wider variety of interests and job opportunities, which could lead to more wage variability at higher levels of education.\n",
    "- People with very low levels of education have fewer opportunities and often must work at the minimum wage\n",
    "- This serves to reduce wage variability at low education levels.\n",
    "\n",
    "Whether Assumption SLR.5 holds is an empirical issue, and we will later show how to test Assumption SLR.5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**$Var(wage|educ)$ increasing with $educ$**\n",
    "\n",
    "\n",
    "<br>  \n",
    "\n",
    "\n",
    "<center><img src=\"figs/wool_2_9.png\" width=\"500\"/> \n",
    "    \n",
    "\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Variances of the OLS Estimators (Self Study)**\n",
    "\n",
    "The following slides present formulas on \n",
    "\n",
    "1. how to derive the variance of $\\hat{\\beta_1}$ and \n",
    "2. how to relate this concept to estimated standard erros of the estimated parameters, \n",
    "    - which is a preciseness measure\n",
    "\n",
    "We leave these slides for self-study, as we come back to this concept in lecture 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Variances of the OLS Estimators (Self Study)**\n",
    "\n",
    "- With the homoscedasticity assumption in place we can prove the following:\n",
    "\n",
    "Under assumptions SLR.1 through SLR.5,\n",
    "\n",
    "\\begin{equation*}\n",
    "Var(\\hat{\\beta_1})=\\frac{\\sigma^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}=\\sigma^2/SST_x\n",
    "\\end{equation*}\n",
    "\n",
    "which is conditional on the sample values {$x_i,...,x_n$}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**PROOF (Self Study):** \n",
    "\n",
    "\\begin{equation*}\n",
    "Var(\\hat{\\beta_1})=(1/STT_x)^2Var\\left(\\sum_{i=1}^n(x_i-\\bar{x})u_i\\right)=(1/STT_x)^2\\left(\\sum_{i=1}^n(x_i-\\bar{x})^2Var(u_i)\\right)=(1/STT_x)^2\\left(\\sum_{i=1}^n(x_i-\\bar{x})^2\\sigma^2)\\right) \n",
    "\\end{equation*}  \n",
    "\n",
    "\n",
    "\n",
    "because $Var(u_i)=\\sigma^2$ for all $i$, and  \n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "(1/STT_x)^2\\left(\\sum_{i=1}^n(x_i-\\bar{x})^2\\sigma^2)\\right)=\\sigma^2(1/STT_x)^2\\left(\\sum_{i=1}^n(x_i-\\bar{x})^2\\right)=\\sigma^2(1/STT_x)^2SST_x=\\sigma^2/SST_x\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Variances of the OLS Estimators (Self Study)**\n",
    "\n",
    "- It is easy to summarize how the variance $Var(\\hat{\\beta}_1)$ depends on the error variance, $\\sigma^2$, and the total variation in {$x_1,...,x_n$}, $SST_x$.\n",
    "- First, the larger the error variance, the larger is $Var(\\hat{\\beta}_1)$.\n",
    "- This makes sense because more variation in the unobservables affecting $y$ makes it more difficult to precisely estimate $\\beta_1$.\n",
    "- On the other hand, more variability in the independent variable is preferred: as the variability in the $x_i$ increases, the variance of $\\beta_1$ decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**(Self Study ctnd.)**\n",
    "\n",
    "- This also makes intuitive sense because the more spread out is the sample of independent variables, the easier it is to trace out the relationship between $E(y|x)$ and $x$; that is, the easier it is to estimate $\\beta_1$.\n",
    "- If there is little variation in the $x_i$, then it can be hard to pinpoint how $E(y|x)$ varies with $x$.\n",
    "- As the sample size increases, so does the total variation in the $x_i$.\n",
    "- Therefore, a larger sample size results in a smaller variance for $\\beta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Estimating the Error Variance (Self Study)**\n",
    "\n",
    "- Formula $Var(\\hat{\\beta_1})=\\frac{\\sigma^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}=\\sigma^2/SST_x$ allows us to isolate the factors that contribute to $Var(\\hat{\\beta}_1)$\n",
    "- But: these formulas are unknown, except in the extremely rare case that $\\sigma^2$ is known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**(Self Study ctnd.)**\n",
    "\n",
    "- Nevertheless, we can use the data to estimate $\\sigma_2$, which then allows us to estimate $Var(\\hat{\\beta}_1)$\n",
    "- The unbiased estimator of $\\sigma^2$ is\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\sigma}^2=\\frac{1}{n-2}\\sum_{i=1}^n\\hat{u}_i^2=SSR/(n-2)\n",
    "\\end{equation*}\n",
    "\n",
    "where the $n-2$ is an adjustment of the degrees of freedom, because we estimate the two parameters $\\beta_0$ and $\\beta_1$ for constructing the residuals.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**(Self Study ctnd.)**\n",
    "\n",
    "- Thus, we can get an unbiased estimator of $Var(\\hat{\\beta}_1)$ (and $Var(\\hat{\\beta}_0)$) by plugging $\\sigma^2$ into \n",
    "\n",
    "\\begin{equation*}\n",
    "Var(\\hat{\\beta_1})=\\frac{\\sigma^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}=\\sigma^2/SST_x\n",
    "\\end{equation*}\n",
    "\n",
    "- Later, we will need estimators of the standard deviations of $\\hat{\\beta}_1$ (and $\\hat{\\beta}_0$), which requires estimating $\\sigma$\n",
    "- The natural estimator of $\\sigma$ is $\\hat\\sigma$=$\\sqrt{\\hat{\\sigma}^2}$ and is called the **standard error of the regression**\n",
    "- Interpretation of the estimate $\\hat{\\sigma}^2$: \n",
    "    - Estimate of the standard deviation in the unobservable affecting $y$, or\n",
    "    - Estimate of the standard deviation in $y$ after the effect of $x$ has been taken out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Standard Errors of Parameter Estimates**\n",
    "\n",
    "- The primary relevance for $\\hat{\\sigma}^2$ is in using it to estimate the stadard deviations of $\\hat{\\beta}_1$ (and $\\hat{\\beta}_0$):\n",
    "\n",
    "\\begin{equation*}\n",
    "se(\\hat{\\beta_1})=\\hat{\\sigma}/\\sqrt{SST_x}=\\hat{\\sigma}/\\left (\\sum_{i=1}^n(x_i-\\bar{x})^2\\right )^{1/2}\n",
    "\\end{equation*}\n",
    "\n",
    "- This is called the **standard error of $\\hat{\\beta}_1$**\n",
    "- Standard errors are an important concept and used to construct test statistics and confidence intervals in econometric procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression on a Binary Explanatory Variable ##\n",
    "\n",
    "Our discussion so far has centered on the case where the explanatory variable, $x$, has quantitative meanings.\n",
    "\n",
    "- E.g. examples include years of schooling or return on equity for a firm.\n",
    "- We know how to interpret the slope coefficient in each case.\n",
    "- We also discussed interpretation of the slope coefficient when we use the logarithmic transformations of the explained variable, the explanatory variable, or both.\n",
    "\n",
    "\n",
    "Simple regression can also be applied to the case where $x$ is a **binary variable**, often called a **dummy variable** in the context of regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Regression on a Binary Explanatory Variable**  \n",
    "\n",
    "As the name binary variable suggests, $x$ takes on only two values, zero and one.\n",
    "\n",
    "- These two values are used to put each unit in the population into one of two groups represented by $x=0$ and $x=1$.\n",
    "- For example, we can use a binary variable to describe whether a worker participates in a job training program.\n",
    "- In the spirit of giving our variables descriptive names, we might use _train_ to indicate participation:\n",
    "    - _train_=1 means a person participates;\n",
    "    - _train_=0 means the person does not.\n",
    "\n",
    "Given a data set, we add an $i$ subscript, as usual, so $train_i$ indicates job training status for a randomly drawn person $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Regression on a Binary Explanatory Variable**  \n",
    "\n",
    "If we have a dependent or response variable, $y$, what does it mean to have a simple regression equation when $x$ is binary?\n",
    "- Consider again the equation  \n",
    "\n",
    "\\begin{equation*}\n",
    "y=\\beta_0+\\beta_1x+u\n",
    "\\end{equation*}\n",
    "\n",
    "- But: now $x$ is a binary variable.\n",
    "- If we impose the zero conditional mean assumption SLR.4 then we obtain, just as before:\n",
    "\n",
    "\\begin{equation*}\n",
    "E(y|x)=\\beta_0+\\beta_1x+E(u|x)=\\beta_0+\\beta_1x\n",
    "\\end{equation*}\n",
    "\n",
    "- The only difference now is that $x$ can take on only two values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Regression on a Binary Explanatory Variable**  \n",
    "\n",
    "- By plugging the values zero and one into the above equation, it is easily seen that\n",
    "\n",
    "\\begin{equation*}\n",
    "E(y|x=0)=\\beta_0\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "E(y|x=1)=\\beta_0+\\beta_1\n",
    "\\end{equation*}\n",
    "\n",
    "and thus\n",
    "\n",
    "\\begin{equation*}\n",
    "\\beta_1=E(y|x=1)-E(y|x=0)\n",
    "\\end{equation*}\n",
    "\n",
    "- Thus: $\\beta_1$ is the difference in the average value of y over the subpopulations with $x=1$ and $x=0$.\n",
    "\n",
    "As with all simple regression analyses, this difference can be descriptive or, in a case discussed in the next subsection, $\\beta_1$ can be a causal effect of an intervention or a program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Regression on a Binary Explanatory Variable**\n",
    "\n",
    " **Example:**\n",
    "- Suppose that every worker in an hourly wage industry is put into one of two racial categories: white (or Caucasian) and nonwhite. (Clearly this is a very crude way to categorize race, but it has been used in some contexts.)\n",
    "- Define the variable _white=1_ if a person is classified as\n",
    "Caucasian and zero otherwise.\n",
    "- Let _wage_ denote hourly wage  \n",
    "\n",
    "Then  \n",
    "\n",
    "\\begin{equation*}\n",
    "\\beta_1=E(wage|white=1)-E(wage|white=0)\n",
    "\\end{equation*}  \n",
    "\n",
    "is the difference in average wages between whites\n",
    "and nonwhites.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results.summary(): \n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           np.log(wage)   R-squared:                       0.002\n",
      "Model:                            OLS   Adj. R-squared:                 -0.000\n",
      "Method:                 Least Squares   F-statistic:                    0.7936\n",
      "Date:                Tue, 15 Sep 2020   Prob (F-statistic):              0.373\n",
      "Time:                        14:03:56   Log-Likelihood:                -413.04\n",
      "No. Observations:                 526   AIC:                             830.1\n",
      "Df Residuals:                     524   BIC:                             838.6\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      1.6303      0.024     66.620      0.000       1.582       1.678\n",
      "nonwhite      -0.0680      0.076     -0.891      0.373      -0.218       0.082\n",
      "==============================================================================\n",
      "Omnibus:                       15.730   Durbin-Watson:                   1.816\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               16.589\n",
      "Skew:                           0.389   Prob(JB):                     0.000250\n",
      "Kurtosis:                       3.389   Cond. No.                         3.33\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "wage1 = woo.dataWoo('wage1')\n",
    "\n",
    "reg = smf.ols(formula='np.log(wage) ~ nonwhite', data=wage1)\n",
    "results = reg.fit()\n",
    "print(f'results.summary(): \\n{results.summary()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Regression on a Binary Explanatory Variable**  \n",
    "\n",
    "- The interpretation is as follows:\n",
    "    - Let $\\bar{y}_0$ be the average of the $y_i$ with $x_i=0$ and $\\bar{y}_1$ the average when $x_i=1$.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\beta}_0=\\bar{y}_0\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\beta}_1=\\bar{y}_1-\\bar{y}_0\n",
    "\\end{equation*}\n",
    "\n",
    "- For example, in the wage/race example, if we run the regression $wage_i$ on $white_i$, $i$=1, . . . , n, then $\\hat{\\beta}_0=\\bar{wage}_0$ is the average hourly wage for nonwhites, and $\\hat{\\beta}_1=\\bar{wage}_1-\\bar{wage}_0$ is the difference in average hourly wages between whites and nonwhites.\n",
    "\n",
    "So does this mean that there is wage discrimination?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "No, it does not necessarily measure wage discrimination because there are many legitimate reasons wages can differ, and some of those — such as education levels — could differ, on average, by race.\n",
    "\n",
    "- We will address this issue in Lecture 4: Multiple Linear Regressions\n",
    "\n",
    "(Besides this the effect is not significant, we will discuss also later what this mean.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Regression on a Binary Explanatory Variable**  \n",
    "\n",
    "- Thus: \n",
    "    - As with any simple regression analysis, the main concern is the zero conditional mean assumption, SLR.4.\n",
    "    - In many cases, this condition will fail because $x$ is systematically related to other factors that affect $y$, and those other factors are necessarily part of $u$.\n",
    "    - We alluded to this above in discussing differences in average hourly wage by race: education and workforce experience are two variables that affect hourly wage that could systematically differ by race."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression on a Binary Explanatory Variable**  \n",
    "\n",
    "- As another example, suppose we have data on SAT scores for students who did and did not take at least one SAT preparation course.\n",
    "- Then $x$ is a binary variable, say, course, and the outcome variable is the SAT score, sat.\n",
    "- The decision to take the preparation course could be systematically related to other factors that are predictive of SAT scores, such as family income and parents’ education.\n",
    "- A comparison of average SAT scores between the two groups is unlikely to uncover the causal effect of the preparation course. The framework covered in the next subsection allows us to determine the special circumstances under which simple regression can uncover a causal effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Counterfactual Outcomes, Causality, and Policy Analysis##  \n",
    "\n",
    "- We have now introduced the notion of a binary explanatory variable\n",
    "\n",
    "$\\rightarrow$ good time to provide a formal framework for studying counterfactual or potential outcomes.\n",
    "\n",
    "- We are particularly interested in defining a **causal effect** or **treatment effect**.\n",
    "- Simplest case: we are interested in evaluating an intervention or policy that has only two states of the world: \n",
    "    - a unit is subjected to the intervention\n",
    "    - and one that is not.\n",
    "- In other words: those not subject to the intervention or new policy act as a **control group** and those subject to the intervention as the **treatment group**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Counterfactual Outcomes, Causality, and Policy Analysis**  \n",
    "\n",
    "- Using the **counterfactual outcomes** framework introduced earlier, for each unit $i$ in the population we assume there are outcomes in both states of the world, $y_i(0)$ and $y_i(1)$.\n",
    "- We will never observe any unit in both states of the world but we imagine each unit in both states.\n",
    "- For example: \n",
    "    - In studying a job training program, a person does or does not participate.\n",
    "    - Then $y_i(0)$ is earnings if person $i$ does not  participate and $y_i(1)$ is labor earnings if $i$ does participate.\n",
    "\n",
    "- These outcomes are well defined before the program is even implemented.\n",
    "   \n",
    "- The causal effect, somewhat more commonly called the treatment effect, of the intervention for unit $i$ is simply the difference between the two potential outcomes.\n",
    "\n",
    "\\begin{equation*}\n",
    "te_i=y_i(1)-y_i(0)\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Counterfactual Outcomes, Causality, and Policy Analysis**\n",
    "\n",
    "- $te_i$ is not observed for any unit $i$ because it depends on both counterfactuals. \n",
    "- $te_i$ can be negative, zero, or positive (it could be that the causal effect is negative for some units and positive for\n",
    "others.)\n",
    "- We cannot hope to estimate $te_i$ for each unit $i$.\n",
    "- Instead, the focus is typically on the **average treatment effect (ATE)**\n",
    "- The ATE is simply the average of the treatment effects across the entire population.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Counterfactual Outcomes, Causality, and Policy Analysis**\n",
    "\n",
    "\n",
    "- We can write the ATE parameter as\n",
    "\n",
    "\\begin{equation*}\n",
    "\\tau_{ate}=E[te_i]=E[y_i(1)-y_i(0)]=E[y_i(1)]-E[y_i(0)]\n",
    "\\end{equation*} \n",
    "\n",
    "- For each unit $i$ let $x_i$ be the program participation status — a binary variable.\n",
    "- Then the observed outcome, $y_i$, can be written as\n",
    "\n",
    "\\begin{equation*}\n",
    "y_i=(1-x_i)y_i(0)+x_iy_i(1)\n",
    "\\end{equation*} \n",
    "\n",
    "which is just shorthand for $y_i=y_i(0)$ if $x_i=0$ and $y_i=y_i(1)$ if $x_1=1$.\n",
    "- This equation precisely describes why, given a random sample from the population, we observe only one of $y_i(0)$ and $y_i(1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Counterfactual Outcomes, Causality, and Policy Analysis**\n",
    "\n",
    "- To see how to estimate the average treatment effect, it is useful to rearrange the above equation:\n",
    "\n",
    "\\begin{equation*}\n",
    "y_i=y_i(0)+[y_i(1)-y_i(0)]x_i\n",
    "\\end{equation*} \n",
    "\n",
    "Now impose a simple (and, usually, unrealistic) constant treatment effect. Namely, for all $i$,\n",
    "\n",
    "\\begin{equation*}\n",
    "y_i(1)=\\tau+y_i(0)\n",
    "\\end{equation*} \n",
    "\n",
    "or $\\tau=y_i(1)-y_i(0)$, thus\n",
    "\n",
    "\\begin{equation*}\n",
    "y_i=y_i(0)+\\tau x_i\n",
    "\\end{equation*} \n",
    "\n",
    "- Now write $y_i(0)=\\alpha_0+u_i(0)$, where, by definition, $\\alpha_0=E[y_i(0)]$ and $E[u_i(0)]=0$. Plugging this in gives\n",
    "\n",
    "\\begin{equation*}\n",
    "y_i=\\alpha_0+\\tau x_i+u_i(0)\n",
    "\\end{equation*} \n",
    "\n",
    "If we define $beta_0=\\alpha_0$, $\\beta_1=\\tau$ and$u_i=u_i(0)$ then the equation becomes the one we already know:\n",
    "\n",
    "\\begin{equation*}\n",
    "y_i=\\beta_0+\\beta_1x_i+u_i\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\beta_1$ is the treatment (or causal) effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Counterfactual Outcomes, Causality, and Policy Analysis**  \n",
    "\n",
    "- As usual, we assume random sampling (SLR.2), and SLR.3 holds provided we have some treated units and some control units, a basic requirement.\n",
    "- It is pretty clear we cannot learn anything about the effect of the intervention if all sampled units are in the control group or all are in the treatment group.\n",
    "- If $x_i$ is independent of $u_i(0)$ then $E[u_i(0)|x_i]=0$, so that SLR.4 also hold and in this case the estimator is unbiased.\n",
    "- The assumption that $x_i$ is independent of $u_i(0)$ is the same as $x_i$ is independent of $y_i(0)$.\n",
    "- This assumption can be guaranteed only under **random assignment**, whereby units are assigned to the treatment and control groups using a randomization mechanism that ignores any features of the individual units.\n",
    "- For example, in evaluating a job training program, random assignment occurs if a coin is flipped to determine whether a worker is in the control group or treatment group.\n",
    "- While we have show that the estimator is unbiased in case of random assignment of the tratment in case of a constant treatment effect, the same is also true if this assumption is relaxed, i.e. if treatment effects vary individually.\n",
    "- In the next chapter, we will see how multiple regression analysis can be used when pure random assignment does not hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Counterfactual Outcomes, Causality, and Policy Analysis**\n",
    "\n",
    "- Random assignment is the hallmark of a randomized controlled trial (RCT), which has long been considered the gold standard for determing whether medical interventions have causal effects. - In recent years, RCTs have become more popular in certain fields in economics, such as development economics and behavioral economics.\n",
    "- Unfortunately, RCTs can be very expensive to implement, and in many cases randomizing subjects into control and treatment groups raises ethical issues. (For example, if giving low-income families access to free health care improves child health outcomes then randomizing some families into the control group means those children will have, on average, worse health outcomes than they could have otherwise.)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
